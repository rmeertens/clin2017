{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers\n",
    "from tensorflow.contrib.rnn.python.ops import core_rnn_cell_impl\n",
    "from tensorflow.contrib.seq2seq.python.ops import attention_decoder_fn\n",
    "from tensorflow.contrib.seq2seq.python.ops import decoder_fn as decoder_fn_lib\n",
    "from tensorflow.contrib.seq2seq.python.ops import seq2seq\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import rnn\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.python.ops import variables\n",
    "from tensorflow.python.platform import test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_decoders_train_inference():\n",
    "    decoder_fn_inference = attention_decoder_fn.attention_decoder_fn_inference(output_fn=output_fn,\n",
    "                                                                                   encoder_state=encoder_state,\n",
    "                                                                                  attention_keys=attention_keys,\n",
    "                                                                                  attention_values=attention_values,\n",
    "                                                                                  attention_score_fn=attention_score_fn,\n",
    "                                                                                  attention_construct_fn=attention_construct_fn,\n",
    "                                                                                  embeddings=decoder_embeddings,\n",
    "                                                                                  start_of_sequence_id=start_of_sequence_id,\n",
    "                                                                                  end_of_sequence_id=end_of_sequence_id,\n",
    "                                                                                  maximum_length=decoder_sequence_length - 1,\n",
    "                                                                                  num_decoder_symbols=num_decoder_symbols,\n",
    "                                                                                  dtype=dtypes.int32)\n",
    "    decoder_fn_train = attention_decoder_fn.attention_decoder_fn_train(\n",
    "      encoder_state=encoder_state,\n",
    "      attention_keys=attention_keys,\n",
    "      attention_values=attention_values,\n",
    "      attention_score_fn=attention_score_fn,\n",
    "      attention_construct_fn=attention_construct_fn)\n",
    "    return decoder_fn_train,decoder_fn_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "encoder_embedding_size = 64\n",
    "decoder_embedding_size = 64\n",
    "encoder_hidden_size = 1024\n",
    "decoder_hidden_size = encoder_hidden_size\n",
    "input_sequence_length = 10\n",
    "decoder_sequence_length = 10\n",
    "num_decoder_symbols = 10000\n",
    "\n",
    "\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "start_of_sequence_id = GO_ID\n",
    "end_of_sequence_id = EOS_ID\n",
    "\n",
    "def process_lines(line,frline):\n",
    "    numbers_in = [int(a) for a in line.split()]\n",
    "    numbers_out = [int(a) for a in frline.split()]\n",
    "    return numbers_in,numbers_out\n",
    "    \n",
    "def add_padding(lines,maxlen):\n",
    "    new_lines = []\n",
    "    for line in lines:\n",
    "        line = line+[EOS_ID]\n",
    "        new_l = line + [PAD_ID]*(maxlen-len(line))\n",
    "        new_lines.append(new_l)\n",
    "    return new_lines\n",
    "def get_some_data():\n",
    "    maxlen = 10000\n",
    "    with open('giga-fren.release2.fixed.en.ids10000') as fin:\n",
    "        with open('giga-fren.release2.fixed.fr.ids10000') as fout:\n",
    "            en_sentences = []\n",
    "            fr_sentences = []\n",
    "            for i,enline in enumerate(fin):\n",
    "                if i>maxlen:\n",
    "                    break\n",
    "                frline = fout.readline()\n",
    "                numbers_in,numbers_out = process_lines(enline,frline)\n",
    "                if len(numbers_in)+1 < input_sequence_length and len(numbers_out)+1 < decoder_sequence_length:\n",
    "                    en_sentences.append(numbers_in)\n",
    "                    fr_sentences.append(numbers_out)\n",
    "    return en_sentences,fr_sentences\n",
    "en_sentences, fr_sentences = get_some_data()\n",
    "en_sentences = add_padding(en_sentences,input_sequence_length)\n",
    "fr_sentences = add_padding(fr_sentences,decoder_sequence_length)\n",
    "en_sentences = [a[::-1] for a in en_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define inputs/outputs to model\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "inputs_placeholder = tf.placeholder(tf.int32, [batch_size,input_sequence_length])\n",
    "decoder_inputs_placeholder = tf.placeholder(tf.int32,[batch_size,input_sequence_length])\n",
    "\n",
    "encoder_embeddings = tf.Variable(\n",
    "    tf.random_uniform([num_decoder_symbols, encoder_embedding_size], -1.0, 1.0))\n",
    "\n",
    "decoder_embeddings = tf.Variable(\n",
    "    tf.random_uniform([num_decoder_symbols, decoder_embedding_size], -1.0, 1.0))\n",
    "\n",
    "\n",
    "inputs = tf.transpose(inputs_placeholder)\n",
    "decoder_inputs = tf.transpose(decoder_inputs_placeholder)\n",
    "inputs = tf.nn.embedding_lookup(encoder_embeddings, inputs)\n",
    "decoder_inputs = tf.nn.embedding_lookup(decoder_embeddings, decoder_inputs)\n",
    "# inputs = constant_op.constant(\n",
    "#     0.5,\n",
    "#     shape=[input_sequence_length, batch_size, encoder_embedding_size])\n",
    "# decoder_inputs = constant_op.constant(\n",
    "#     0.4,\n",
    "#     shape=[decoder_sequence_length, batch_size, decoder_embedding_size])\n",
    "decoder_length = constant_op.constant(\n",
    "    decoder_sequence_length, dtype=dtypes.int32, shape=[batch_size,])\n",
    "\n",
    "# attention\n",
    "attention_option = \"luong\"  # can be \"bahdanau\"\n",
    "\n",
    "with variable_scope.variable_scope(\"rnn\") as scope:\n",
    "  # Define model\n",
    "  cell = core_rnn_cell_impl.GRUCell(decoder_hidden_size)\n",
    "  encoder_outputs, encoder_state = rnn.dynamic_rnn(\n",
    "      cell=cell,\n",
    "      inputs=inputs,\n",
    "      dtype=dtypes.float32,\n",
    "      time_major=True,\n",
    "      scope=scope)\n",
    "\n",
    "#   attention_states: size [batch_size, max_time, num_units]\n",
    "attention_states = array_ops.transpose(encoder_outputs, [1, 0, 2])\n",
    "\n",
    "\n",
    "with variable_scope.variable_scope(\"decoder\") as scope:\n",
    "  # Prepare attention\n",
    "  (attention_keys, attention_values, attention_score_fn,\n",
    "   attention_construct_fn) = (attention_decoder_fn.prepare_attention(\n",
    "       attention_states, attention_option, decoder_hidden_size))\n",
    "\n",
    "  # setting up weights for computing the final output\n",
    "  def create_output_fn():\n",
    "\n",
    "    def output_fn(x):\n",
    "      return layers.linear(x, num_decoder_symbols, scope=scope)\n",
    "\n",
    "    return output_fn\n",
    "\n",
    "  output_fn = create_output_fn()\n",
    "  \n",
    "  decoder_fn_train,decoder_fn_inference = get_decoders_train_inference()\n",
    "  # Train decoder\n",
    "  decoder_cell = core_rnn_cell_impl.GRUCell(decoder_hidden_size)\n",
    "  (decoder_outputs_train, decoder_state_train, _) = (\n",
    "      seq2seq.dynamic_rnn_decoder(\n",
    "          cell=decoder_cell,\n",
    "          decoder_fn=decoder_fn_train,\n",
    "          inputs=decoder_inputs,\n",
    "          sequence_length=decoder_length,\n",
    "          time_major=True,\n",
    "          scope=scope))\n",
    "  decoder_outputs_train = output_fn(decoder_outputs_train)\n",
    "  # Setup variable reuse\n",
    "  scope.reuse_variables()\n",
    "  # Inference decoder\n",
    "  (decoder_outputs_inference, decoder_state_inference, _) = (\n",
    "      seq2seq.dynamic_rnn_decoder(\n",
    "          cell=decoder_cell,\n",
    "          decoder_fn=decoder_fn_inference,\n",
    "          time_major=True,\n",
    "          scope=scope))\n",
    "\n",
    "\n",
    "weights = [tf.ones_like(labels_t, dtype=tf.float32) for labels_t in tf.unstack(decoder_outputs_train)]\n",
    "\n",
    "my_loss = tf.nn.seq2seq.sequence_loss(decoder_inputs_placeholder, decoder_outputs_train, weights, num_decoder_symbols)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(my_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "variables.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for _ in range(10000):\n",
    "    in_x = en_sentences[:batch_size]\n",
    "    in_y = fr_sentences[:batch_size]\n",
    "    (not_important,the_loss,decoder_outputs_train_res, decoder_state_train_res) = sess.run([optimizer,my_loss,decoder_outputs_train, decoder_state_train],feed_dict={inputs_placeholder:in_x ,decoder_inputs_placeholder: in_y })\n",
    "    #(decoder_outputs_inference_res, decoder_state_inference_res) = sess.run([decoder_outputs_inference, decoder_state_inference])\n",
    "    #print((decoder_sequence_length, batch_size,num_decoder_symbols))\n",
    "    #print(decoder_outputs_train_res.shape)\n",
    "    #print(my_loss)\n",
    "    print(my_loss.eval(feed_dict={inputs_placeholder:in_x ,decoder_inputs_placeholder: in_y }))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vocabulary_in = [a.strip() for a in open('vocab10000.from')]\n",
    "vocabulary_out = [a.strip() for a in open('vocab10000.to')]\n",
    "\n",
    "print(decoder_outputs_train_res.shape)\n",
    "a = decoder_outputs_train_res.transpose(1,0,2)\n",
    "print(a.shape)\n",
    "for index_sentences,sentence in enumerate(a):\n",
    "    print(sentence.shape)\n",
    "    string_sentence = \"\"\n",
    "    for index_now in in_x[index_sentences]:\n",
    "        if index_now == EOS_ID:\n",
    "            break\n",
    "        string_sentence += (vocabulary_in[index_now]) + \" \"\n",
    "    print(string_sentence)\n",
    "    string_sentence = \"\"\n",
    "    activations = []\n",
    "    activations_of_inspected = []\n",
    "    for whoo_i,word in enumerate(sentence):\n",
    "        index_now = np.argmax(word)\n",
    "        if index_now == EOS_ID:\n",
    "            break\n",
    "        string_sentence += (vocabulary_out[index_now]) + \" \"\n",
    "        activations.append(word[index_now])\n",
    "        activations_of_inspected.append(word[in_y[index_sentences][whoo_i]])\n",
    "    print(string_sentence)\n",
    "    string_sentence = \"\"\n",
    "    for index_now in in_y[index_sentences]:\n",
    "        if index_now == EOS_ID:\n",
    "            break\n",
    "        string_sentence += (vocabulary_out[index_now]) + \" \"\n",
    "    print(string_sentence)\n",
    "    print(activations)\n",
    "    print(activations_of_inspected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_cell(prevc,prevh,xt,hidden_size):\n",
    "    concatenated = tf.concat(0,prevh,xt)\n",
    "    W1 = tf.Variable(rng.randn(),shape=[batch,input_size,hidden_size])\n",
    "    W2 = tf.Variable(rng.randn(),shape=[batch,input_size,hidden_size])\n",
    "    W3 = tf.Variable(rng.randn(),shape=[batch,input_size,hidden_size])\n",
    "    tempC =  tf.element_mul(tf.matmul(concatenated,W1),prevc)\n",
    "    add_to_c = tf.element_mul(tf.matmul(concatenated,W2),tf.nn.tanh(concatenated))\n",
    "    new_c = tempC + add_to_c\n",
    "    new_h = tf.element_mul(tf.nn.tanh(new_c),tf.matmul(concatenated,W3))\n",
    "    return new_c,new_h\n",
    "\n",
    "\n",
    "\n",
    "forget_weights =   ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
