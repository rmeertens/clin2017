{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why split file in subwords\n",
    "Neural machine translation works by encoding each words as an integer, embedding it, and putting it through a recurrent neural network. \n",
    "The problem with this approach is that it is difficult to determine an embedding for infrequent words (like, retinoscopy).\n",
    "\n",
    "Recently researchers discovered that splitting words in subwords helps with neural machine translation. \n",
    "Especially in languages that can concatenate words (Dutch, German, ..) splitting these concatenated words to more commonly occuring words helps to generalise translation. \n",
    "Another advantage is dealing with abbreviations (like CIA, or NSA). By splitting these down to their core letters a network will learn to translate every abbreviation. \n",
    "\n",
    "\n",
    "\n",
    "# Current state of the art\n",
    "\n",
    "A commonly used script is that of Sennich et al. (https://github.com/rsennrich/subword-nmt). They use bite-pair encoding to reduce the amount of words in a text. \n",
    "\n",
    "# Simple splitter\n",
    "As a nice evening exercise I wanted to implement a slightly different approach: the one of finding common prefixes in words and encoding a file using that. \n",
    "\n",
    "Here is a short writeup of my stuff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['g', 'r', 'e', 'a', 't', 'gr', 're', 'ea', 'at', 'gre', 'rea', 'eat', 'grea', 'reat', 'great']\n"
     ]
    }
   ],
   "source": [
    "def get_subwords_in_word(word):\n",
    "    subwords = []\n",
    "    for subword_length in range(0,len(word)):\n",
    "        for i in range(len(word)-subword_length):\n",
    "            subwords.append(word[i:i+subword_length+1])\n",
    "    return subwords\n",
    "\n",
    "assert len(get_subwords_in_word(\"great\")[0])==1\n",
    "assert get_subwords_in_word(\"great\")[-1]=='great'\n",
    "\n",
    "print(get_subwords_in_word(\"great\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the frequency of subwords\n",
    "Now that we know what subwords are in a word, let's get all subwords in a file and sort them by frequency.\n",
    "Note that sometimes single letters (like X) don't occur that often. It is important to add each single-letter subword to prevent this problem. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_subword_and_frequencies(words):\n",
    "    subword_frequency = dict()\n",
    "    for word in words:\n",
    "        for subword in get_subwords_in_word(word):\n",
    "            if subword not in subword_frequency:\n",
    "                subword_frequency[subword]=0\n",
    "            subword_frequency[subword]+=1\n",
    "\n",
    "    subwords_and_frequency = list(subword_frequency.items())\n",
    "    subwords_and_frequency = [(b,a) for a,b in subwords_and_frequency]\n",
    "    return subwords_and_frequency\n",
    "\n",
    "def get_n_subwords(words,first_n):\n",
    "    subwords_and_frequency = get_subword_and_frequencies(words)\n",
    "    subwords_and_frequency.sort(reverse=True)\n",
    "    \n",
    "    # initialise the subwords to all single-letter subwords\n",
    "    subwords = [word for freq,word in subwords_and_frequency if len(word)==1]\n",
    "    \n",
    "    # add multi-length subwords till we have the first n \n",
    "    for freq,word in subwords_and_frequency:\n",
    "        if len(word)>1:\n",
    "            subwords.append(word)\n",
    "        if len(subwords)==first_n:\n",
    "            break\n",
    "    \n",
    "    return subwords\n",
    "\n",
    "# old_subwords = get_n_subwords(old_words,300)\n",
    "# old_subwords.sort()\n",
    "# new_subwords = get_n_subwords(new_words,300)\n",
    "# new_subwords.sort()\n",
    "# print(old_subwords)\n",
    "# print(new_subwords)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', 't', 'a', 'o', 'n', 'i', 's', 'r', 'h', 'd', 'l', 'c', 'u', 'm', 'f', 'w', 'g', 'p', 'y', ',', 'b', '.', 'v', 'k', '\"', 'I', '-', 'T', 'A', \"'\", 'x', 'P', 'S', 'H', 'N', 'W', 'M', 'C', 'B', 'E', 'R', '1', '_', 'j', 'F', 'q', '!', 'O', '?', 'D', 'z', ';', 'G', '0', '2', 'L', '8', '3', '4', 'Y', '5', '9', '6', '7', ':', 'V', '=', ')', '(', 'K', 'U', 'J', '#', 'X', '*', ']', '[', '|', 'Q', 'Z', '/', '$', '+', '@', '&', '%', '>', '~', '^', '<', 'he', 'th', 'in', 'the', 'er', 'an', 're', 'on', 'nd', 'at', 'en', 'ed', 'es', 'is', 'and', 'or', 'ng', 'of', 'to', 'te', 'ha', 'it', 'ou', 'ti', 'ar', 'hi', 'as', 'st', 'se', 'nt', 'ing', 'le', 'al', 'me', 've', 'ne', 'ro', 'ri', 'ea', 'de', 'co', 'ce', 'io', 'll', 'ra', 'om', 'ic', 'ion', 'be', 'li', 'ho', 'ur', 'ch', 'la', 'si', 'wa', 'el', 'ut', 'ss', 'ma', 'us', 'ad', 'ly', 'no', 'ta', 'her', 'ent', 'tio', 'tion', 'ca', 'di', 'wh', 'fo', 'ot', 'ow', 'wi', 'pe', 'nc', 'lo', 'un', 'sh', 'il', 'hat', 'ns', 'ac', 'rs', 'ai', 'ie', 'im', 'ol', 'so', 'ec', 'ee', 'his', 'ere', 'id', 'et', 'ul', 'for', 'tr', 'we', 'em', 'tha', 'pr', 'ct', 'os', 'ter', 'sa', 'ni', 'ge', 'mo', 'ith', 'that', 'ir', 'su', 'Th', 'rt', 'ati', 'ry', 'was', 'po', 'oo', 'ld', 'wit', 'ov', 'with', 'all', 'pa', 'ver', 'ess', 'ay', 'nce', 'gh', 'ts', 'res', 'mi', 'The', 'fe', 'na', 'ev', 'ted', 'ke', 'fi', 'ate', 'am', 'fr', 'ig', 'ther', 'not', 'pl', 'ia', 'tu', 'ers', 'atio', 'ation', 'vi', 'bl', 'con', 'av', 'men', 'mp', 'thi', 'do', 'rr', 'tt', 'one', 'iv', 'ab', 'yo', 'wo', 'ap', 'ons', 'had', 'ci', 'ag', 'rea', 'rd', 'oun', 'op', 'ep', 'bo', 'rm', 'fa', 'bu', 'ex', 'ei', 'der', 'eve', 'rin', 'gr', 'you', 'ht', 'ome', 'by', 'ty', 'ear', 'if', 'out', 'ght', 'hou', 'hin', 'ey', 'rom', 'int', 'est', 'com', 'are', 'sp', 'ain', 'him']\n",
      "1 loop, best of 3: 7.83 s per loop\n"
     ]
    }
   ],
   "source": [
    "# Let's test what subwords we get for the book of Sherlock Holmes\n",
    "# which can be downloaded here: http://norvig.com/big.txt\n",
    "\n",
    "import re\n",
    "\n",
    "def only_lowercase_words(text): return re.findall(r'\\w+', text.lower())\n",
    "def only_words(text): return re.findall(r'\\w+', text)\n",
    "\n",
    "_WORD_SPLIT = re.compile(\"([.,!?\\\"':;)( ])\")\n",
    "def basic_tokenizer(sentence):\n",
    "  \"\"\"Very basic tokenizer: split the sentence into a list of tokens.\"\"\"\n",
    "  words = []\n",
    "  for space_separated_fragment in sentence.strip().split():\n",
    "    words.extend(_WORD_SPLIT.split(space_separated_fragment))\n",
    "  return [w for w in words if w]\n",
    "\n",
    "\n",
    "def get_n_subwords_filename(filename,first_n):\n",
    "    tokens_here = []\n",
    "    with open(filename) as file_input:\n",
    "        for line in file_input:\n",
    "            tokens_here.extend(basic_tokenizer(line))\n",
    "    words = tokens_here\n",
    "    subwords_and_frequency = get_subword_and_frequencies(words)\n",
    "    subwords_and_frequency.sort(reverse=True)\n",
    "    \n",
    "    # initialise the subwords to all single-letter subwords\n",
    "    subwords = [word for freq,word in subwords_and_frequency if len(word)==1]\n",
    "    \n",
    "    # add multi-length subwords till we have the first n \n",
    "    for freq,word in subwords_and_frequency:\n",
    "        if len(word)>1:\n",
    "            subwords.append(word)\n",
    "        if len(subwords)==first_n:\n",
    "            break\n",
    "    \n",
    "    return subwords\n",
    "def words(text):\n",
    "    return basic_tokenizer(text)\n",
    "    \n",
    "# Let's see how fast we can read the data, and how fast we can subword it\n",
    "# %timeit new_subwords = get_n_subwords(words(open('big.txt').read()),300)\n",
    "new_subwords = get_n_subwords_filename('big.txt',300)\n",
    "print(new_subwords)\n",
    "%timeit get_n_subwords_filename('big.txt',300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are several normal English words in there (with, the, here) which are common to the english language. \n",
    "\n",
    "Let's try to tokenize a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ther@@ e he@@ ll@@ o \n"
     ]
    }
   ],
   "source": [
    "def get_subworded(word,subwords):\n",
    "    if word==\"\":\n",
    "        return []\n",
    "    for i in range(len(word),-1,-1):\n",
    "        try:\n",
    "            if word[:i] in subwords:\n",
    "                base = [word[:i]]\n",
    "                base.extend(get_subworded(word[i:],subwords))\n",
    "                return base\n",
    "        except:\n",
    "            print(\"nooo exception!\")\n",
    "            print(word)\n",
    "SUBWORD_SEPERATOR = \"@@ \"\n",
    "def subword_sentence(sentence,subwords):\n",
    "    tokens = sentence.split()\n",
    "    subworded_sentence = \"\"\n",
    "    for word in tokens:\n",
    "        subwords_here = get_subworded(word,subwords)\n",
    "        first_subwords = subwords_here[:-1]\n",
    "        last_subword = subwords_here[-1]\n",
    "        for subw in first_subwords:\n",
    "            subworded_sentence += subw + SUBWORD_SEPERATOR\n",
    "        subworded_sentence += last_subword + \" \"\n",
    "    return subworded_sentence\n",
    "\n",
    "print(subword_sentence(\"there hello\",new_subwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing split methods\n",
    "Let's see how this split method compares to the one of Sennich et al. \n",
    "\n",
    "The video card I can use for this small experiment is a NVideo Quadro 4400 or something, so not that well suited for neural networks. This makes me run only 2 experiments: 1500 subwords and 3000 subwords. \n",
    "\n",
    "So the experiment setup is:\n",
    "\n",
    "                1500 subwords.... 3000 subwords\n",
    "                \n",
    "No subwords\n",
    "\n",
    "Sennich subwords\n",
    "\n",
    "My subword method\n",
    "\n",
    "The dataset I will use is the WMT 2014 dataset by default used by translate.py. The scores I will compare are the perplexity on the testset after 20.000 iterations. The network has seen 1.280.000 sentences at that moment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename_input = '/home/roland/tensorflow/tensorflow/models/rnn/translate/newwmtdatadir/giga-fren.release2.fixed.fr'\n",
    "n_subwords = 1500\n",
    "subwords = get_n_subwords_filename(filename_input,n_subwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\x01', '\\x02', '\\x03', '\\x04', '\\x05', '\\x06', '\\x07', '\\x08', '\\x0e', '\\x0f', '\\x10', '\\x11', '\\x12', '\\x13', '\\x14', '\\x15', '\\x16', '\\x17', '\\x18', '\\x19', '\\x1a', '\\x1b', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\x7f', '\\x80', '\\x81', '\\x82', '\\x83', '\\x84', '\\x86', '\\x87', '\\x88', '\\x89', '\\x8a', '\\x8b', '\\x8c', '\\x8d', '\\x8e', '\\x8f', '\\x90', '\\x91', '\\x92', '\\x93', '\\x94', '\\x95', '\\x96', '\\x97', '\\x98', '\\x99', '\\x9a', '\\x9b', '\\x9c', '\\x9d', '\\x9e', '\\x9f', '¡', '¢', '£', '¤', '¥', '¦', '§', '¨', '©', 'ª', '«', '¬', '\\xad', '®', '¯', '°', '±', '²', '³', '´', 'µ', '¶', '·', '¸', '¹', 'º', '»', '¼', '½', '¾', '¿', 'À', 'Á', 'Â', 'Ã', 'Ä', 'Å', 'Æ', 'Ç', 'È', 'É', 'Ê', 'Ë', 'Ì', 'Í', 'Î', 'Ï', 'Ð', 'Ñ', 'Ò', 'Ó', 'Ô', 'Õ', 'Ö', '×', 'Ø', 'Ù', 'Ú', 'Û', 'Ü', 'Ý', 'Þ', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', '÷', 'ø', 'ù', 'ú', 'û', 'ü', 'ý', 'þ', 'ÿ', 'ć', 'ĉ', 'č', 'Ď', 'ď', 'đ', 'ē', 'Ė', 'ė', 'Ę', 'ę', 'ě', 'ĝ', 'ğ', 'ĥ', 'Ĩ', 'ĩ', 'ı', 'Ĺ', 'ĺ', 'ł', 'ń', 'ň', 'Ō', 'ő', 'Œ', 'œ', 'Ŕ', 'ŕ', 'Ş', 'ş', 'Š', 'š', 'ū', 'ů', 'Ÿ', 'ż', 'Ž', 'ž', 'Ƒ', 'ƒ', 'Ȅ', 'ʹ', 'ʺ', 'ʼ', 'ˆ', 'ˇ', '˘', '˙', '˚', '˛', '˜', '̀', '́', '̊', ';', 'Γ', 'Δ', 'Ε', 'Η', 'Θ', 'Λ', 'Μ', 'Π', 'Σ', 'Τ', 'Υ', 'Φ', 'Ψ', 'Ω', 'α', 'β', 'γ', 'δ', 'ε', 'ζ', 'η', 'θ', 'ι', 'κ', 'λ', 'μ', 'ν', 'ξ', 'ο', 'π', 'ρ', 'ς', 'σ', 'τ', 'υ', 'φ', 'χ', 'ψ', 'ω', 'ϑ', 'ϒ', 'ϕ', 'Є', 'І', 'В', 'Д', 'Е', 'З', 'И', 'Й', 'К', 'Л', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ц', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'ц', 'ч', 'ш', 'щ', 'ы', 'ь', 'ю', 'я', 'ё', 'є', 'і', 'ї', 'ا', 'ب', 'ة', 'ح', 'ذ', 'ر', 'ص', 'ع', 'غ', 'ف', 'ل', 'ه', 'ي', '٭', 'ท', 'ย', 'ไ', 'Ế', 'ừ', '‐', '‑', '–', '—', '―', '‘', '’', '‚', '‛', '”', '„', '†', '‡', '•', '‣', '…', '\\u202b', '\\u202c', '‰', '′', '″', '‹', '›', '⁄', '\\u206a', '\\u206c', '\\u206d', '\\u206e', '\\u206f', '₤', '€', 'ℜ', '℡', '™', 'Ω', '⅓', '⅔', '⅛', '⅜', '←', '↑', '→', '↓', '↔', '⇒', '⇓', '⇔', '∀', '∂', '∆', '∈', '∏', '∑', '−', '∕', '∗', '√', '∝', '∞', '∫', '∼', '≈', '≠', '≡', '≤', '≥', '⊂', '⊃', '⊇', '⋅', '⌠', '⌧', '⎛', '⎜', '⎝', '⎞', '⎟', '⎠', '⎡', '⎢', '⎣', '⎤', '⎥', '⎦', '⎮', '⎯', '①', '②', '─', '│', '▀', '■', '□', '▪', '▫', '▲', '►', '▼', '◆', '◊', '○', '●', '◗', '◦', '☐', '☼', '♦', '✄', '✏', '✓', '✔', '✖', '✗', '✘', '✜', '✤', '✳', '❍', '❏', '❑', '❒', '❖', '❚', '❶', '➔', '➘', '➙', '➝', '➟', '➠', '➢', '➤', '➥', '➪', '➮', '。', '〈', '一', '三', '上', '下', '不', '世', '业', '中', '为', '也', '了', '事', '于', '产', '代', '任', '会', '但', '作', '使', '供', '保', '信', '借', '债', '做', '先', '全', '公', '关', '内', '况', '出', '划', '创', '别', '券', '力', '加', '务', '包', '十', '即', '及', '发', '变', '口', '只', '可', '司', '同', '告', '和', '品', '商', '在', '场', '多', '大', '它', '完', '实', '容', '富', '对', '展', '已', '市', '年', '府', '建', '当', '息', '情', '成', '投', '报', '担', '括', '拿', '按', '据', '掌', '控', '提', '撤', '收', '政', '文', '断', '新', '时', '是', '普', '更', '有', '本', '机', '来', '款', '此', '比', '汉', '活', '版', '用', '界', '的', '益', '相', '着', '知', '神', '票', '种', '站', '精', '系', '级', '网', '者', '而', '能', '般', '行', '表', '解', '计', '议', '证', '评', '语', '请', '财', '责', '贷', '资', '跃', '转', '迅', '通', '速', '销', '键', '靠', '경', '과', '관', '글', '금', '는', '니', '다', '당', '대', '됩', '로', '료', '면', '명', '미', '번', '불', '사', '서', '성', '수', '시', '신', '액', '요', '용', '우', '유', '율', '의', '이', '입', '자', '적', '전', '정', '제', '착', '청', '초', '하', '한', '호', '화', '환', '\\ue024', '\\ue031', '\\ue033', '\\ue044', '\\ue053', '\\ue06a', '\\ue06e', '\\ue070', '\\ue83a', '\\uf015', '\\uf020', '\\uf024', '\\uf028', '\\uf029', '\\uf02a', '\\uf02b', '\\uf030', '\\uf038', '\\uf03c', '\\uf041', '\\uf042', '\\uf04c', '\\uf050', '\\uf066', '\\uf06a', '\\uf06d', '\\uf06f', '\\uf071', '\\uf072', '\\uf080', '\\uf081', '\\uf08f', '\\uf090', '\\uf0a7', '\\uf0b1', '\\uf0b7', '\\uf0ba', '\\uf0c9', '\\uf0fc', '\\uf6da', '\\uf6db', '\\uf6fa', '\\uf724', '\\uf730', '\\uf731', '\\uf732', '\\uf733', '\\uf734', '\\uf735', '\\uf736', '\\uf737', '\\uf738', '\\uf739', '\\uf73f', '\\uf761', '\\uf762', '\\uf763', '\\uf764', '\\uf765', '\\uf766', '\\uf767', '\\uf768', '\\uf769', '\\uf76a', '\\uf76b', '\\uf76c', '\\uf76d', '\\uf76e', '\\uf76f', '\\uf770', '\\uf771', '\\uf772', '\\uf773', '\\uf774', '\\uf775', '\\uf776', '\\uf778', '\\uf779', '\\uf77a', '\\uf7b4', '\\uf7e0', '\\uf7e8', '\\uf7e9', '\\uf7ee', '\\uf7f4', '\\uf818', '\\uf8e3', '\\uf8e4', '\\uf8e7', '\\uf8e9', '\\uf8eb', '\\uf8ec', '\\uf8ed', '\\uf8ee', '\\uf8ef', '\\uf8f0', '\\uf8f1', '\\uf8f2', '\\uf8f3', '\\uf8f4', '\\uf8f5', '\\uf8f6', '\\uf8f7', '\\uf8f8', '\\uf8f9', '\\uf8fa', '\\uf8fb', '\\uf8fc', '\\uf8fd', '\\uf8fe', 'ﬀ', 'ﬁ', 'ﬂ', 'ﬃ', '\\ufdd3', '，', '：', 'ｷ', '�', '00', '20', 'Ca', 'Co', 'La', 'Le', '__', 'ab', 'ac', 'ad', 'ag', 'ai', 'al', 'am', 'an', 'ap', 'ar', 'as', 'at', 'au', 'av', 'ba', 'bi', 'bl', 'bo', 'br', 'bu', 'ca', 'cc', 'ce', 'ch', 'ci', 'cl', 'co', 'cr', 'ct', 'cu', 'da', 'de', 'di', 'do', 'dr', 'du', 'dé', 'd’', 'ea', 'ec', 'ef', 'ei', 'el', 'em', 'en', 'ep', 'er', 'es', 'et', 'eu', 'ev', 'ex', 'ez', 'fa', 'fe', 'ff', 'fi', 'fo', 'fr', 'ga', 'ge', 'gi', 'gn', 'gr', 'gé', 'ha', 'he', 'ho', 'ia', 'ib', 'ic', 'id', 'ie', 'if', 'ig', 'il', 'im', 'in', 'io', 'ip', 'iq', 'ir', 'is', 'it', 'iv', 'iè', 'ié', 'je', 'la', 'le', 'li', 'll', 'lo', 'ls', 'lt', 'lu', 'lé', 'l’', 'ma', 'mb', 'me', 'mi', 'mm', 'mo', 'mp', 'mu', 'mé', 'na', 'nc', 'nd', 'ne', 'nf', 'ng', 'ni', 'nn', 'no', 'ns', 'nt', 'nu', 'nv', 'né', 'ob', 'oc', 'od', 'of', 'og', 'oi', 'ol', 'om', 'on', 'op', 'or', 'os', 'ot', 'ou', 'pa', 'pe', 'ph', 'pi', 'pl', 'po', 'pp', 'pr', 'pt', 'pu', 'pé', 'qu', 'ra', 'rc', 'rd', 're', 'rg', 'ri', 'rm', 'rn', 'ro', 'rr', 'rs', 'rt', 'rv', 'ré', 'sa', 'sc', 'se', 'si', 'so', 'sp', 'ss', 'st', 'su', 'sé', 'ta', 'te', 'th', 'ti', 'to', 'tr', 'ts', 'tt', 'tu', 'té', 'ua', 'ub', 'uc', 'ud', 'ue', 'ui', 'ul', 'um', 'un', 'up', 'ur', 'us', 'ut', 'uv', 'ux', 'va', 've', 'vi', 'vo', 'vr', 'xp', 'ys', 'èr', 'ès', 'éc', 'éd', 'ée', 'ég', 'él', 'ém', 'én', 'ép', 'ér', 'és', 'ét', 'év', 'êt', '’a', '’e', '’i', '’u', '200', 'Can', 'Les', '___', 'abl', 'acc', 'act', 'ada', 'adi', 'age', 'ain', 'air', 'ais', 'ait', 'ale', 'ali', 'amm', 'ana', 'anc', 'and', 'ang', 'ani', 'ann', 'ans', 'ant', 'app', 'ara', 'art', 'ass', 'ate', 'ati', 'aut', 'aux', 'ava', 'ave', 'ble', 'bli', 'bre', 'can', 'cat', 'cer', 'ces', 'cha', 'che', 'cia', 'cie', 'com', 'con', 'cou', 'cte', 'cti', 'cul', 'dan', 'des', 'die', 'dis', 'don', 'dre', 'dui', 'eau', 'ech', 'ect', 'ell', 'emb', 'eme', 'emp', 'enc', 'end', 'ens', 'ent', 'enu', 'erc', 'erm', 'ern', 'ers', 'ert', 'erv', 'ess', 'est', 'ett', 'eur', 'eux', 'fai', 'fic', 'fin', 'for', 'gne', 'gra', 'her', 'ica', 'ice', 'ici', 'ide', 'ien', 'ier', 'ieu', 'ifi', 'ign', 'ili', 'ill', 'imp', 'ina', 'inc', 'ine', 'ini', 'ins', 'int', 'ion', 'iqu', 'ire', 'isa', 'ise', 'iss', 'ist', 'ita', 'ite', 'iti', 'its', 'itu', 'ité', 'ive', 'ièr', 'lan', 'lat', 'lem', 'les', 'leu', 'lie', 'lis', 'lit', 'lle', 'lus', 'l’a', 'mai', 'man', 'mat', 'men', 'mes', 'min', 'mis', 'mme', 'mpl', 'mpo', 'nad', 'nal', 'nan', 'nat', 'nce', 'nci', 'nda', 'nde', 'ndi', 'nem', 'nes', 'nis', 'nne', 'nné', 'nou', 'nse', 'nsi', 'nta', 'nte', 'nti', 'ntr', 'nts', 'nté', 'née', 'odu', 'ogr', 'oin', 'oir', 'ois', 'oit', 'omm', 'omp', 'onc', 'ond', 'onn', 'ons', 'ont', 'ori', 'orm', 'ort', 'our', 'ous', 'out', 'ouv', 'par', 'pas', 'per', 'plu', 'por', 'pos', 'pou', 'pre', 'pri', 'pro', 'pré', 'que', 'qui', 'rai', 'ral', 'ram', 'ran', 'rap', 'rat', 'rch', 'rec', 'rem', 'ren', 'rep', 'res', 'rie', 'ris', 'rit', 'rme', 'rod', 'rog', 'ron', 'rta', 'rte', 'rti', 'rés', 'san', 'sat', 'sem', 'sen', 'ser', 'ses', 'sio', 'sit', 'son', 'sou', 'ssa', 'sse', 'ssi', 'sta', 'sti', 'str', 'sur', 'tai', 'tan', 'tat', 'ten', 'ter', 'tes', 'teu', 'tic', 'tie', 'tif', 'tio', 'tiq', 'tit', 'tiv', 'tou', 'tra', 'tre', 'tri', 'tte', 'tur', 'tés', 'uel', 'ues', 'uit', 'ult', 'une', 'ure', 'urs', 'ute', 'uti', 'utr', 'uve', 'van', 'ven', 'ver', 'vis', 'ère', 'ées', 'éra', 'éri', 'ése', 'éta', 'été', 'Cana', '____', 'able', 'adie', 'aire', 'amme', 'anad', 'ance', 'ande', 'ants', 'arti', 'atio', 'cati', 'cher', 'comm', 'comp', 'cons', 'cont', 'ctio', 'dans', 'dien', 'ecti', 'elle', 'emen', 'ence', 'ense', 'enta', 'ente', 'enti', 'entr', 'ents', 'esti', 'ette', 'eurs', 'form', 'icat', 'ient', 'ille', 'inte', 'ions', 'ique', 'ires', 'itio', 'ités', 'ière', 'leme', 'leur', 'lles', 'mand', 'mati', 'ment', 'nada', 'nadi', 'neme', 'nnée', 'nter', 'ntre', 'ogra', 'omme', 'onne', 'ontr', 'ouve', 'part', 'plus', 'port', 'pour', 'prod', 'prés', 'ques', 'rati', 'rodu', 'rogr', 'rtic', 'rése', 'sant', 'sent', 'serv', 'sion', 'sont', 'tair', 'tant', 'tati', 'teur', 'tion', 'tiqu', 'tive', 'tres', 'ture', 'utre', 'vent', 'Canad', 'adien', 'aires', 'anada', 'anadi', 'artic', 'ation', 'catio', 'comme', 'ction', 'ement', 'entre', 'icati', 'iques', 'ition', 'lemen', 'ments', 'nadie', 'nemen', 'parti', 'produ', 'ratio', 'taire', 'tatio', 'tions', 'tique', 'Canada', 'anadie', 'ations', 'cation', 'ements', 'icatio', 'lement', 'nadien', 'nement', 'ration', 'tation', 'anadien', 'ication']\n"
     ]
    }
   ],
   "source": [
    "subwords.sort()\n",
    "subwords.sort(key=len)\n",
    "print(subwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=CAA36398F918BEFE49955BB13B4F7082?doi=10.1.1.117.1928&rep=rep1&type=pdf\n",
    "# apparently according to this paper it would be better to focus on the END of the word instead... \n",
    "# ... would be nice to try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename_output = '/home/roland/tensorflow/tensorflow/models/rnn/translate/newwmtdatadir/giga-fren.release2.fixed.fr.subworded_1500'\n",
    "with open(filename_input) as input_file, open(filename_output,'w') as output_file:\n",
    "    line_now = 0\n",
    "    for line in input_file:\n",
    "        line_now +=1\n",
    "        if line_now %250==1:\n",
    "            print(str(line_now))\n",
    "       # print(line)\n",
    "        output_line = subword_sentence(line,subwords)\n",
    "        output_file.write(output_line+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ª', '²', '³', 'µ', '¹', 'º', '¼', '½', '¾', 'À', 'Á', 'Â', 'Ã', 'Ä', 'Å', 'Æ', 'Ç', 'È', 'É', 'Ê', 'Ë', 'Ì', 'Í', 'Î', 'Ï', 'Ð', 'Ñ', 'Ò', 'Ó', 'Ô', 'Õ', 'Ö', 'Ø', 'Ù', 'Ú', 'Û', 'Ü', 'Ý', 'Þ', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', 'ø', 'ù', 'ú', 'û', 'ü', 'ý', 'þ', 'ÿ', 'ā', 'ă', 'ć', 'Č', 'č', 'Ē', 'ě', 'ı', 'ň', 'ő', 'Œ', 'œ', 'Ş', 'ş', 'Š', 'š', 'ū', 'Ÿ', 'Ž', 'ž', 'ƒ', 'ʹ', 'ʺ', 'ʼ', 'ˆ', 'ˇ', 'Α', 'Γ', 'Δ', 'Ε', 'Ζ', 'Η', 'Θ', 'Λ', 'Μ', 'Π', 'Σ', 'Τ', 'Υ', 'Φ', 'Χ', 'Ψ', 'Ω', 'α', 'β', 'γ', 'δ', 'ε', 'ζ', 'η', 'θ', 'ι', 'κ', 'λ', 'μ', 'ν', 'ξ', 'ο', 'π', 'ρ', 'ς', 'σ', 'τ', 'υ', 'φ', 'χ', 'ψ', 'ω', 'ϑ', 'ϕ', 'Є', 'І', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'З', 'И', 'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч', 'Ш', 'Ы', 'Ь', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'х', 'ц', 'ч', 'ш', 'щ', 'ы', 'ь', 'ю', 'я', 'ё', 'є', 'і', 'ї', 'أ', 'إ', 'ا', 'ب', 'ة', 'ت', 'ج', 'ح', 'د', 'ذ', 'ر', 'س', 'ش', 'ص', 'ط', 'ع', 'غ', 'ف', 'ق', 'ل', 'م', 'ن', 'ه', 'و', 'ي', '٠', 'ท', 'ย', 'ไ', 'ᐃ', 'ᐅ', 'ᐊ', 'ᐋ', 'ᐱ', 'ᐳ', 'ᐴ', 'ᐸ', 'ᑉ', 'ᑎ', 'ᑏ', 'ᑐ', 'ᑑ', 'ᑕ', 'ᑖ', 'ᑦ', 'ᑭ', 'ᑯ', 'ᑲ', 'ᒃ', 'ᒋ', 'ᒍ', 'ᒐ', 'ᒡ', 'ᒥ', 'ᒦ', 'ᒧ', 'ᒪ', 'ᒻ', 'ᓂ', 'ᓃ', 'ᓄ', 'ᓅ', 'ᓇ', 'ᓈ', 'ᓐ', 'ᓕ', 'ᓗ', 'ᓘ', 'ᓚ', 'ᓪ', 'ᓯ', 'ᓰ', 'ᓱ', 'ᓲ', 'ᓴ', 'ᔨ', 'ᔪ', 'ᔫ', 'ᔭ', 'ᔮ', 'ᔾ', 'ᕆ', 'ᕈ', 'ᕋ', 'ᕌ', 'ᕐ', 'ᕕ', 'ᕗ', 'ᕙ', 'ᕝ', 'ᕿ', 'ᖃ', 'ᖅ', 'ᖏ', 'ᖑ', 'ᖓ', 'ᖕ', 'ế', 'ệ', 'Ω', '⅓', '⅔', '⅛', '⅜', '⅝', '①', '②', '❶', '一', '三', '上', '下', '不', '世', '业', '中', '为', '也', '了', '于', '产', '代', '任', '会', '但', '作', '使', '供', '保', '信', '借', '债', '做', '先', '全', '公', '关', '内', '况', '出', '划', '创', '别', '券', '力', '加', '务', '包', '十', '即', '及', '发', '变', '口', '只', '可', '司', '同', '告', '和', '品', '商', '在', '场', '多', '大', '它', '完', '实', '容', '富', '对', '展', '已', '市', '年', '府', '建', '当', '息', '情', '成', '投', '报', '担', '括', '拿', '按', '据', '掌', '控', '提', '撤', '收', '政', '文', '断', '新', '时', '是', '更', '有', '本', '机', '来', '款', '此', '比', '版', '用', '界', '的', '益', '相', '着', '知', '神', '票', '种', '站', '精', '系', '级', '网', '者', '而', '能', '般', '行', '表', '解', '计', '议', '证', '评', '请', '财', '责', '贷', '资', '跃', '转', '迅', '通', '速', '销', '键', '靠', '가', '경', '계', '과', '관', '국', '글', '금', '기', '는', '능', '니', '다', '당', '대', '됩', '란', '로', '료', '만', '면', '명', '미', '번', '불', '사', '서', '성', '송', '수', '시', '신', '액', '어', '요', '용', '우', '유', '율', '은', '의', '이', '입', '자', '적', '전', '접', '정', '제', '좌', '직', '착', '청', '초', '타', '표', '하', '한', '합', '행', '호', '화', '환', 'ﬀ', 'ﬁ', 'ﬂ', 'ﬃ', 'ﬄ', '00', '19', '20', '99', 'Ca', 'Co', 'De', 'Fo', 'He', 'In', 'Ma', 'Pr', 'Re', 'Se', 'St', 'Th', '__', 'ab', 'ac', 'ad', 'af', 'ag', 'ai', 'ak', 'al', 'am', 'an', 'ap', 'ar', 'as', 'at', 'au', 'av', 'ax', 'ay', 'ba', 'be', 'bi', 'bl', 'bo', 'br', 'bs', 'bu', 'by', 'ca', 'cc', 'ce', 'ch', 'ci', 'ck', 'cl', 'co', 'cr', 'ct', 'cu', 'cy', 'da', 'de', 'di', 'do', 'dr', 'ds', 'du', 'ea', 'ec', 'ed', 'ee', 'ef', 'eg', 'ei', 'el', 'em', 'en', 'ep', 'eq', 'er', 'es', 'et', 'ev', 'ew', 'ex', 'ey', 'fa', 'fe', 'ff', 'fi', 'fo', 'fr', 'fu', 'ga', 'ge', 'gh', 'gi', 'gn', 'go', 'gr', 'gu', 'ha', 'he', 'hi', 'ho', 'hr', 'ht', 'ia', 'ib', 'ic', 'id', 'ie', 'if', 'ig', 'il', 'im', 'in', 'io', 'ip', 'ir', 'is', 'it', 'iv', 'iz', 'je', 'ke', 'ki', 'la', 'ld', 'le', 'li', 'll', 'lo', 'ls', 'lt', 'lu', 'ly', 'ma', 'mb', 'me', 'mi', 'mm', 'mo', 'mp', 'ms', 'mu', 'na', 'nc', 'nd', 'ne', 'nf', 'ng', 'ni', 'nm', 'nn', 'no', 'ns', 'nt', 'nu', 'nv', 'ny', 'oa', 'ob', 'oc', 'od', 'of', 'og', 'oi', 'ol', 'om', 'on', 'oo', 'op', 'or', 'os', 'ot', 'ou', 'ov', 'ow', 'pa', 'pe', 'ph', 'pi', 'pl', 'po', 'pp', 'pr', 'pt', 'pu', 'qu', 'ra', 'rc', 'rd', 're', 'rg', 'ri', 'rk', 'rl', 'rm', 'rn', 'ro', 'rp', 'rr', 'rs', 'rt', 'ru', 'rv', 'ry', 'sa', 'sc', 'se', 'sh', 'si', 'sm', 'so', 'sp', 'ss', 'st', 'su', 'ta', 'te', 'th', 'ti', 'tl', 'tm', 'to', 'tr', 'ts', 'tt', 'tu', 'tw', 'ty', 'ua', 'ub', 'uc', 'ud', 'ue', 'ug', 'ui', 'ul', 'um', 'un', 'up', 'ur', 'us', 'ut', 'va', 've', 'vi', 'wa', 'we', 'wh', 'wi', 'wo', 'xp', 'ye', 'yo', 'ys', '200', 'Can', 'Com', 'Con', 'Hea', 'Pro', 'The', '___', 'abl', 'acc', 'ach', 'act', 'ada', 'ade', 'adi', 'age', 'ail', 'ain', 'ake', 'ali', 'all', 'als', 'alt', 'ame', 'ana', 'anc', 'and', 'ang', 'ani', 'ans', 'ant', 'any', 'app', 'arc', 'ard', 'are', 'ari', 'art', 'ary', 'ase', 'ass', 'ast', 'ate', 'ati', 'ave', 'ber', 'bil', 'ble', 'bli', 'but', 'cal', 'can', 'cat', 'cen', 'cer', 'ces', 'cha', 'che', 'cia', 'cie', 'clu', 'com', 'con', 'cou', 'cre', 'cti', 'cto', 'cts', 'cul', 'dat', 'ded', 'den', 'der', 'des', 'dev', 'dia', 'din', 'dis', 'dit', 'duc', 'eal', 'ear', 'eas', 'eat', 'eci', 'eco', 'ect', 'edi', 'edu', 'eed', 'een', 'egi', 'eir', 'ele', 'eli', 'ell', 'elo', 'eme', 'enc', 'end', 'ene', 'ens', 'ent', 'equ', 'era', 'ere', 'eri', 'erm', 'ern', 'ers', 'ert', 'erv', 'ese', 'esp', 'ess', 'est', 'ete', 'eve', 'evi', 'exp', 'fer', 'ffe', 'ffi', 'fic', 'for', 'fro', 'gen', 'gra', 'han', 'har', 'has', 'hat', 'hav', 'hea', 'hei', 'her', 'hes', 'hic', 'hin', 'his', 'hou', 'ial', 'ian', 'iat', 'ica', 'ice', 'ich', 'ici', 'ide', 'ien', 'ies', 'ifi', 'igh', 'ign', 'ili', 'ill', 'ime', 'imp', 'ina', 'inc', 'ind', 'ine', 'inf', 'ing', 'ini', 'ins', 'int', 'ion', 'ire', 'ish', 'isi', 'iss', 'ist', 'ita', 'ite', 'ith', 'iti', 'its', 'itu', 'ity', 'ive', 'ivi', 'jec', 'kin', 'lan', 'lar', 'lat', 'lea', 'lec', 'les', 'lic', 'lin', 'lis', 'lit', 'lle', 'lly', 'lop', 'low', 'lth', 'lud', 'man', 'mar', 'mat', 'mbe', 'men', 'mer', 'min', 'mit', 'mmu', 'mon', 'mor', 'mpl', 'mpo', 'mun', 'nad', 'nal', 'nat', 'nce', 'nci', 'ncl', 'nda', 'nde', 'ndi', 'ned', 'ner', 'nes', 'nfo', 'nge', 'nic', 'nin', 'nit', 'nme', 'not', 'nsi', 'nst', 'nsu', 'nta', 'nte', 'nti', 'ntr', 'nts', 'odu', 'ogr', 'oli', 'oll', 'ome', 'omm', 'omp', 'ona', 'ond', 'one', 'ons', 'ont', 'ood', 'ope', 'ora', 'ord', 'ore', 'ori', 'ork', 'orm', 'ort', 'ose', 'ost', 'ote', 'oth', 'oul', 'oun', 'our', 'ous', 'out', 'ove', 'ovi', 'par', 'pec', 'pen', 'per', 'pla', 'ple', 'pli', 'pon', 'por', 'pos', 'ppl', 'ppo', 'pre', 'pri', 'pro', 'qui', 'rac', 'ral', 'ram', 'ran', 'rat', 'rce', 'rch', 'rea', 'rec', 'red', 'ree', 'reg', 'rel', 'ren', 'rep', 'res', 'rev', 'ria', 'ric', 'rie', 'rin', 'rio', 'rit', 'rma', 'rod', 'rog', 'rom', 'ron', 'rop', 'rou', 'rov', 'rta', 'rti', 'rvi', 'sea', 'sec', 'sed', 'sen', 'ser', 'ses', 'sho', 'sid', 'sin', 'sio', 'sis', 'sit', 'son', 'spe', 'spo', 'sse', 'ssi', 'sta', 'ste', 'sti', 'str', 'sub', 'sup', 'sur', 'tai', 'tal', 'tan', 'tat', 'ted', 'tem', 'ten', 'ter', 'tes', 'tha', 'the', 'thi', 'tho', 'tia', 'tic', 'tie', 'tim', 'tin', 'tio', 'tit', 'tiv', 'tme', 'tor', 'tra', 'tre', 'tri', 'tte', 'tur', 'ual', 'uct', 'ues', 'ula', 'uld', 'ult', 'und', 'uni', 'unt', 'upp', 'ura', 'ure', 'use', 'usi', 'ust', 'uti', 'val', 'vel', 'ven', 'ver', 'ves', 'vic', 'vid', 'vin', 'was', 'wer', 'whi', 'wil', 'wit', 'wor', 'yea', 'you', 'Cana', '____', 'able', 'acti', 'adia', 'ally', 'alth', 'anad', 'ance', 'arch', 'arti', 'ated', 'atio', 'ativ', 'cati', 'cess', 'cial', 'clud', 'comm', 'comp', 'cons', 'cont', 'ctio', 'ctiv', 'ctor', 'dent', 'dian', 'ding', 'duct', 'ealt', 'earc', 'ease', 'ecti', 'elop', 'emen', 'ence', 'enta', 'enti', 'ents', 'eral', 'erat', 'ervi', 'esea', 'esti', 'evel', 'ffic', 'form', 'from', 'gram', 'have', 'heal', 'heir', 'here', 'hich', 'ical', 'icat', 'ices', 'iden', 'ific', 'ilit', 'incl', 'info', 'inte', 'iona', 'ions', 'itie', 'itio', 'ject', 'lati', 'lity', 'mati', 'mber', 'ment', 'mmun', 'mple', 'muni', 'nada', 'nadi', 'nati', 'nclu', 'nder', 'nfor', 'ning', 'niti', 'nmen', 'nter', 'oduc', 'ogra', 'onal', 'ontr', 'orma', 'othe', 'ould', 'ount', 'over', 'ovid', 'part', 'port', 'prod', 'prov', 'rate', 'rati', 'reas', 'rese', 'ring', 'rmat', 'rodu', 'rogr', 'rovi', 'rvic', 'sear', 'serv', 'sing', 'sion', 'spec', 'ssio', 'stan', 'tati', 'tern', 'that', 'thei', 'ther', 'this', 'ties', 'ting', 'tion', 'tive', 'tmen', 'trat', 'ture', 'ulat', 'unde', 'unit', 'velo', 'vern', 'vice', 'vide', 'whic', 'will', 'with', 'work', 'year', 'Canad', '_____', 'adian', 'anada', 'anadi', 'ation', 'ative', 'catio', 'ction', 'ctive', 'ealth', 'earch', 'ectio', 'ement', 'ervic', 'esear', 'evelo', 'forma', 'healt', 'icati', 'inclu', 'infor', 'inter', 'ional', 'ities', 'ition', 'latio', 'matio', 'ments', 'nadia', 'natio', 'nclud', 'nform', 'nment', 'oduct', 'ogram', 'ormat', 'other', 'ovide', 'produ', 'provi', 'ratio', 'rmati', 'roduc', 'rogra', 'rovid', 'rvice', 'searc', 'ssion', 'their', 'tiona', 'tions', 'tment', 'under', 'velop', 'which', 'Canada', 'Canadi', '______', 'anadia', 'ationa', 'ations', 'cation', 'ection', 'ervice', 'esearc', 'evelop', 'format', 'health', 'icatio', 'inform', 'lation', 'mation', 'nadian', 'nation', 'nforma', 'ormati', 'produc', 'provid', 'ration', 'rmatio', 'roduct', 'rogram', 'rovide', 'search', 'tional', 'Canadia', '_______', 'anadian', 'ational', 'esearch', 'formati', 'ication', 'informa', 'nformat', 'ormatio', 'rmation', 'Canadian', '________', 'formatio', 'nformati', 'ormation', '_________', 'formation', 'nformatio', '__________', 'nformation', '___________', '____________', '_____________', '______________', '_______________', '________________']\n"
     ]
    }
   ],
   "source": [
    "subwords.sort()\n",
    "subwords.sort(key=len, reverse=False) # sorts by descending length\n",
    "\n",
    "print(subwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2974d2ff9437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0moutput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_line\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msubword_file_filenames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/roland/tensorflow/tensorflow/models/rnn/translate/newwmtdatadir/giga-fren.release2.fixed.en'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'/home/roland/tensorflow/tensorflow/models/rnn/translate/newwmtdatadir/giga-fren.release2.fixed.en.subworded_1500'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# Let's see how fast we can read the data, and how fast we can subword it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#subword_english_wmt = get_n_subwords(words(open('/home/roland/tensorflow/tensorflow/models/rnn/translate/newwmtdatadir/giga-fren.release2.fixed.en').read()),1500)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-2974d2ff9437>\u001b[0m in \u001b[0;36msubword_file_filenames\u001b[0;34m(filename_input, filename_output, n_subwords)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_input\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0moutput_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubword_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0moutput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_line\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-c3d3c98620da>\u001b[0m in \u001b[0;36msubword_sentence\u001b[0;34m(sentence, subwords)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0msubwords_here\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_subworded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mfirst_subwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubwords_here\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mlast_subword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubwords_here\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msubw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfirst_subwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "def subword_file_filenames(filename_input,filename_output,n_subwords):\n",
    "    subwords = get_n_subwords(words(open(filename_input).read()),n_subwords)    \n",
    "    with open(filename_input) as input_file, open(filename_output,'w') as output_file:\n",
    "        for line in input_file:\n",
    "            output_line = subword_sentence(line,subwords)\n",
    "            output_file.write(output_line+\"\\n\")\n",
    "\n",
    "subword_file_filenames('/home/roland/tensorflow/tensorflow/models/rnn/translate/newwmtdatadir/giga-fren.release2.fixed.en','/home/roland/tensorflow/tensorflow/models/rnn/translate/newwmtdatadir/giga-fren.release2.fixed.en.subworded_1500',1500)\n",
    "# Let's see how fast we can read the data, and how fast we can subword it\n",
    "#subword_english_wmt = get_n_subwords(words(open('/home/roland/tensorflow/tensorflow/models/rnn/translate/newwmtdatadir/giga-fren.release2.fixed.en').read()),1500)\n",
    "#print(subword_english_wmt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "_WORD_SPLIT = re.compile(\"([.,!?\\\"':;)( ])\")\n",
    "\n",
    "old_words = []\n",
    "new_words = []\n",
    "\n",
    "with open(\"clin2017/lexicon.txt\") as lexicon_file:\n",
    "    for two_words in lexicon_file:\n",
    "        old, new = two_words.split()\n",
    "        old_words.append(old)\n",
    "        new_words.append(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_subworded(word,subwords):\n",
    "    if word==\"\":\n",
    "        return []\n",
    "    for i in range(len(word),-1,-1):\n",
    "        try:\n",
    "            if word[:i] in subwords:\n",
    "\n",
    "                base = [word[:i]]\n",
    "                base.extend(get_subworded(word[i:],subwords))\n",
    "                return base\n",
    "        except:\n",
    "            print(word)\n",
    "\n",
    "def get_subworded_words(words,subwords):\n",
    "    subworded = []\n",
    "    for complete_word in words:\n",
    "        subwords_here = get_subworded(complete_word,subwords)        \n",
    "        subworded.append(subwords_here)\n",
    "    return subworded\n",
    "subworded = get_subworded_words(old_words,old_subwords)\n",
    "a = [len(b) for b in subworded]\n",
    "print(max(a))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def basic_tokenizer(sentence):\n",
    "  \"\"\"Very basic tokenizer: split the sentence into a list of tokens.\"\"\"\n",
    "  words = []\n",
    "  for space_separated_fragment in sentence.strip().split():\n",
    "    words.extend(_WORD_SPLIT.split(space_separated_fragment))\n",
    "  return [w for w in words if w]\n",
    "\n",
    "\n",
    "with open(\"clin2017/1637/bible.txt\") as input:\n",
    "    input_lines = [l for l in input]\n",
    "    \n",
    "    #input_words = [w for l in input_lines for w in re.split(_WORD_SPLIT,l) ]\n",
    "    #with open(\"subwords_bible_1637.txt\",'w') as output:\n",
    "        \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old_words = []\n",
    "for l in input_lines:\n",
    "    tokenized_sentence = basic_tokenizer(l)\n",
    "    old_words.extend(tokenized_sentence)\n",
    "old_subwords = get_n_subwords(old_words,300)\n",
    "old_subwords.sort()\n",
    "print(old_subwords)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def subword_file(filename_input,filename_output):\n",
    "    to_output_lines = []\n",
    "    with open(filename_input) as input_file:\n",
    "        input_lines = [l for l in input_file]\n",
    "    print(len(input_lines))\n",
    "    old_words = []\n",
    "    print(\"done with old words\")\n",
    "    for l in input_lines:\n",
    "        tokenized_sentence = basic_tokenizer(l)\n",
    "        old_words.extend(tokenized_sentence)\n",
    "    subwords = get_n_subwords(old_words,300)\n",
    "    subwords.sort()\n",
    "    print('done subwords')\n",
    "    for l in input_lines:\n",
    "        tokenized_sentence = basic_tokenizer(l)\n",
    "        this_line = \"\"\n",
    "\n",
    "        for w in tokenized_sentence:\n",
    "            tokenized_word_here = get_subworded(w,subwords)\n",
    "            for token in tokenized_word_here[:-1]:\n",
    "                this_line+=token+\"@@ \"\n",
    "            this_line+=tokenized_word_here[-1]+ \" \"\n",
    "        to_output_lines.append(this_line)\n",
    "    print(' ready to output')\n",
    "    for line in to_output_lines[:10]:\n",
    "        print(line)\n",
    "    with open(filename_output,'w' ) as output_file:\n",
    "        for line in to_output_lines:\n",
    "            \n",
    "            output_file.write(line)\n",
    "            output_file.write(\"\\n\")\n",
    "subword_file(\"clin2017/1637/bible.txt\",\"subworded_prefix_roland_bible_1637.txt\")\n",
    "subword_file(\"clin2017/1888/bible.txt\",\"subworded_prefix_roland_bible_1888.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(input_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
